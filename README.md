# Apana - LLM Evaluation Technical Test

Welcome! This is a technical challenge designed for candidates applying to our **Design of a performance evaluation system for a financial simulation LLM agent (Fintech)**.

You will build a simple evaluation pipeline to assess the quality of responses generated by a language model, based on reference answers. The test focuses on your **Python skills**, **analytical thinking**, and **understanding of LLMs**.

## Objective

Design and implement a basic framework to evaluate the performance of an LLM when answering financial planning prompts.

You'll work with:
- A small evaluation dataset (`data/eval_set.json`)
- Response generation
- Automatic evaluation metrics

## Tasks

### 1. Load and prepare the evaluation dataset

Write a script or notebook to:
- Load the dataset
- Structure the data into `(prompt, reference_answer)` pairs for evaluation

### 2. Generate answers from the model (20%)

Implement a function:

```python
def generate_response(prompt: str) -> str:
  ...
```

You can use [langchain ChatGoogleGenerativeAI](https://python.langchain.com/docs/integrations/chat/google_generative_ai/) with a [free Gemini API token](https://ai.google.dev/gemini-api/docs/api-key?authuser=1).

### 3. Implement an evaluation function

Design and code a function like:

```python
def evaluate_response(reference: str, generated: str) -> float:
    ...
```

This function should return an evaluation score knowing the reference and the generated answer.

You can choose a simple method such as:
- Similarity based on embeddings (e.g. cosine similarity)
- Heuristic scoring (e.g. keyword overlap)
- LLM as a judge
- Other ?

## How to submit

1. Fork this repository
2. Work in your fork
3. Push your changes to your forked repo
4. Send us the link to your repository via email

Email: contact@apana.ai

Subject: LLM Evaluation Test - [Your Name]

## Resources

- [LangChain Beginner's Guide to Agent Evaluation](https://www.youtube.com/watch?v=_QozKR9eQE8)


