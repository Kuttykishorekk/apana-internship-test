# ğŸ“„ Apana LLM Evaluation System

## ğŸ¯ Project Overview

This project implements a complete evaluation pipeline for assessing a **French financial domain language model**, as part of the Apana technical challenge.

It includes:

- Automated response generation (Gemini)
- Rich evaluation metrics
- Regulatory compliance checks
- Interactive analysis capabilities
- Visual performance reporting

---

## ğŸ› ï¸ Project Structure

```

llm-eval-apana/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ eval_set.json              â† JSON dataset containing prompts and reference answers
â”‚
â”œâ”€â”€ src/                           â† Core logic and evaluation functions
â”‚   â”œâ”€â”€ generate.py                â† LLM response generation using Gemini API (lazy-loaded)
â”‚   â””â”€â”€ evaluate.py                â† All scoring functions: similarity, BLEU, ROUGE, LLM judge, etc.
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_generate.py           â† Unit tests for LLM generation (mocked)
â”‚   â””â”€â”€ test_evaluate.py           â† Unit tests for metrics and scoring functions
â”‚   â””â”€â”€ conftest.py                â† Shared test setup: sets import path
â”‚
â”œâ”€â”€ output/                        â† Stores results from evaluations (e.g., CSVs, logs)
â”‚   â””â”€â”€ results.csv                â† Final scored outputs (optional)
â”‚
â”œâ”€â”€ run_eval.py                    â† Main script that:
â”‚                                     1. Loads dataset
â”‚                                     2. Generates LLM answers
â”‚                                     3. Applies multiple evaluation metrics
â”‚                                     4. Writes results to /output and prints summary
â”‚
â”œâ”€â”€ requirements.txt               â† Python dependencies (LangChain, SentenceTransformers, etc.)
â”‚
â”œâ”€â”€ .env                           â† Your local Gemini API key (not committed)
â”œâ”€â”€ .env.template                  â† Template for `.env`, shared safely in repo
â”‚
â”œâ”€â”€ .gitignore                     â† Excludes .env, venv, __pycache__, etc. from commits
â”‚
â””â”€â”€ README.md                      â† Complete documentation of the system


````

---

## ğŸš€ How to Run

1ï¸âƒ£ **Install dependencies**

```bash
pip install -r requirements.txt
````

2ï¸âƒ£ **Configure environment variables**

Create a `.env` file:

```
GOOGLE_API_KEY=your_gemini_api_key
LANGCHAIN_API_KEY=your_langsmith_api_key
LANGCHAIN_PROJECT=apana-llm-eval
LANGCHAIN_TRACING_V2=true
```

3ï¸âƒ£ **Run the evaluation**

```bash
python run_eval.py
```

---

## âœ… Implemented Features

**1. Dataset Loading**

* Loads prompts and reference answers from `data/eval_set.json`.

**2. Response Generation**

* Gemini 1.5 Flash model via LangChain.
* Custom financial system prompt for domain accuracy.

**3. Evaluation Metrics**

* Cosine similarity
* Keyword overlap
* BLEU score
* ROUGE-1 / ROUGE-2 / ROUGE-L
* LLM-Judge score (0â€“10 scale)
* Self-confidence estimation (0â€“100)
* Hallucination detection + rationale
* Regulatory compliance check (keywords: *AMF*, *ACPR*, etc.)

**4. Result Logging**

* CSV export to `/output`.
* Optional LangSmith logging for advanced review.

---

## ğŸ“Š Evaluation Summary

| Metric                    | Description                                       |
| ------------------------- | ------------------------------------------------- |
| **Cosine Similarity**     | Embedding similarity with reference               |
| **Keyword Overlap**       | Token overlap ratio                               |
| **BLEU Score**            | N-gram similarity                                 |
| **ROUGE Scores**          | Overlap of n-grams and longest common subsequence |
| **LLM-Judge Score**       | Gemini-evaluated quality score                    |
| **Self-confidence**       | Model's self-reported confidence                  |
| **Regulatory Compliance** | Presence of regulatory keywords                   |
| **Hallucination Flag**    | Whether content likely contains hallucination     |
| **Hallucination Reason**  | Explanation of hallucination or correctness       |

---
### âœ… Testing & Validation

This project includes a test suite to validate core functionality such as:

- Dataset loading and input structure
- LLM response generation behavior (mocked)
- Evaluation metrics: BLEU, ROUGE, cosine similarity, keyword overlap, compliance, hallucination, etc.

#### ğŸ”§ Run Tests Locally

Make sure you have all dependencies installed:

```bash
pip install -r requirements.txt
```

Then run the full test suite with:

```bash
pytest -q
```

You should see:

```
............                                                                                                                 [100%]
12 passed in X.XXs
```

#### ğŸ“‚ Test Structure

- `tests/test_evaluate.py` â€“ Unit tests for metric functions and dataset handling  
- `tests/test_generate.py` â€“ Tests Gemini response generation logic (mocked)  
- `tests/conftest.py` â€“ Automatically adds `src/` to `sys.path` for import resolution

#### ğŸ§ª Test Coverage

All critical logic is covered:
- 100% test pass rate
- Validates error-handling and edge cases (empty prompts, hallucinations, API failures)

## ğŸ–¼ï¸ Evaluation Visuals

Below are key charts generated from results:

**1ï¸âƒ£ Number of Hallucinations**

![Hallucination Chart](./Charts/Figure_1.png)

**2ï¸âƒ£ Self-confidence Distribution**

![Self-confidence Chart](./Charts/Figure_2.png)

**3ï¸âƒ£ BLEU vs. Similarity Scatter Plot**

![BLEU vs Similarity](./Charts/Figure_3.png)

---

## ğŸ§  How to Interpret

* **BLEU vs Similarity** shows content fidelity.
* **Hallucinations** indicate factual inconsistencies.
* **Self-confidence** helps assess model calibration.
* **Regulatory compliance** ensures adherence to financial standards.

---

## ğŸŒ LangSmith Integration

If enabled in `.env`, all examples are logged to LangSmith:

* Interactive inspection
* Metadata filtering
* Custom scoring columns

Example fields logged:

* Prompt
* Generated Answer
* All metrics
* Hallucination rationale

ğŸ‘‰ **[View Interactive LangSmith Dashboard](./Charts/langsmithchart.png)

---

## âœ¨ Notes & Next Steps

This system was designed for:

* Easy extension (more metrics, more prompts)
* Compatibility with other models
* Reproducible reporting

Possible enhancements:

* Larger evaluation datasets
* Adversarial prompt testing
* Integration with Streamlit dashboards

---

## ğŸ“¬ Submitted By

* contact to **[contact@kishore](mailto:kishorekumarmourougane@gmail.com)**
* **Subject:** `LLM Evaluation Test - Kishorekumar Mourougane`


